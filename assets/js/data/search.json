[ { "title": "Ansible VAULT with 1password", "url": "/posts/Ansible-vault-1password/", "categories": "Ansible", "tags": "ansible, shell, security", "date": "2022-03-29 00:00:00 +0200", "snippet": "Ansible provides a tool to protect sensitive data in inventories and makes them unusable without encryption password.Documentation for ansible-vault is pretty nice and very helpful, but at some point you will have to think about how to protect your vault password and make it easy to use in your project. Idea in this blog post is to leverage 1password CLI tool to get this password.Install 1password CLI toolHere we will assume installation will be done on MacOS, if you are running a different OS, please check 1password support page for step-by-step installation process.# Install 1password op CLI toolbrew install --cask 1password/tap/1password-cli# Install JQ for JSON parsingbrew install jqThen, configure your op tool to login to your realm:$ op signin &amp;lt;your-realm&amp;gt;.1password.comCreate script to get VAULT passphraseTo do that, we will need your 1password vault ID where your vault password is sotred ($1PASSWORD_VAULT_ID) and the 1password entry where your VAULT passphrase is saved ($1PASSWORD_VAULT_ITEM).Get 1Password VAULT ID op vault lsID NAMExxxxx Personalyyyyy Works...Letâ€™s consider VAULT entry is saved in Works.Get your 1Password Vault entryJust run the following command, assuming your 1password entry is â€˜Ansible Vaultâ€™:op item get &quot;Ansible Vault&quot; --format jsonAnd the result will be something like this:{ &quot;id&quot;: &quot;id1010101010&quot;, &quot;title&quot;: &quot;Ansible Vault&quot;, &quot;version&quot;: 5, &quot;vault&quot;: { &quot;id&quot;: &quot;zzzzzz&quot; }, &quot;category&quot;: &quot;PASSWORD&quot;, &quot;last_edited_by&quot;: &quot;xxx&quot;, &quot;created_at&quot;: &quot;2020-09-01T11:19:19Z&quot;, &quot;updated_at&quot;: &quot;2022-03-28T16:46:59Z&quot;, &quot;sections&quot;: [ { &quot;id&quot;: &quot;add more&quot; }, { &quot;id&quot;: &quot;linked items&quot;, &quot;label&quot;: &quot;Related Items&quot; } ], &quot;fields&quot;: [ { &quot;id&quot;: &quot;password&quot;, &quot;type&quot;: &quot;CONCEALED&quot;, &quot;purpose&quot;: &quot;PASSWORD&quot;, &quot;label&quot;: &quot;password&quot;, &quot;value&quot;: &quot;my-super-safe-password&quot;, &quot;entropy&quot;: 1, &quot;password_details&quot;: { &quot;entropy&quot;: 141, &quot;generated&quot;: true, &quot;strength&quot;: &quot;FANTASTIC&quot; } }, { &quot;id&quot;: &quot;notesPlain&quot;, &quot;type&quot;: &quot;STRING&quot;, &quot;purpose&quot;: &quot;NOTES&quot;, &quot;label&quot;: &quot;notesPlain&quot;, &quot;value&quot;: &quot;&quot; } ]}So now, idea is to use jq to get only password field:# JQ filtering for 1Password version 8jq &#39;.fields[] | select(.id==&quot;password&quot;).value&#39;Finally, we have following command:$ op item get --format json &quot;Ansible Vault&quot; |jq &#39;.fields[] | select(.id==&quot;password&quot;).value&#39; | tr -d &#39;&quot;&#39;my-super-safe-passwordCreate your vaultNow we will put everything in a small bash script to use with ansible# vim ~/bin/op-vault#!/bin/bash# Arista VaultVAULT_ID=&quot;yyyyyy&quot;VAULT_ANSIBLE_NAME=&quot;Ansible Vault&quot;op item get --format json --vault=$VAULT_ID &quot;$VAULT_ANSIBLE_NAME&quot; |jq &#39;.fields[] | select(.id==&quot;password&quot;).value&#39; | tr -d &#39;&quot;&#39;Use your vault in ansibleSo now you can easily encrypt/decrypt using ansible-vault with following command:ansible-vault encrypt_string &#39;ansible&#39; \\ --name &#39;ansible_ssh_pass&#39; \\ --vault-password-file=~/bin/op-vaultansible_ssh_pass: !vault | $ANSIBLE_VAULT;1.1;AES256 38373166646364376632366166383138663238346164386161646536646266633330623966323730 3535633165623632303331393361656663633236366264660a343562636435346630656264353432 36636263613838313836663031326636663461343634333333333663313332343839303131363061 6537653439353630300a626135313461343136626634633764303839653938616264646437623131 3738Encryption successful Note: Donâ€™t forget to accept connection to 1passwordYou can also use it during playbook execution:ansible-playbook --vault-password-file=~/bin/op-vault ...And because it can be painful to type this option all the time, alias comes to the rescue:$ alias ansible-playbook-vault=&#39;ansible-playbook --vault-password-file=~/bin/op-vault&#39;Here you are, it is now easy to protect your sensitive data in your inventories and reduce the risk to loose your passphrase" }, { "title": "Test your network automation with Pytest &amp; Mock", "url": "/posts/test-your-python-network-automation/", "categories": "", "tags": "", "date": "2022-02-18 09:46:00 +0100", "snippet": "Recently, we have been challenged on how we could test our ansible-cvp development to make sure changes are not breaking the code.One of the solution would be to build a test topology we can access from our CI and use it to run ansible content to validate. Regardless the security aspect of such topology, we thought it is not efficient: if your code is broken, you will eat an ansible error message and not a pointer to the python code.So we decided to take the well known software approach: pytest to run unit &amp;amp; system tests. And to use it with no Cloudvision nor EOS devices, we put Mock in place to emulate API endpoint.In this article, we will go through the process we put in place to support ansible-cvp development and go through some basic examples.Create a mock of CVPRAC / CVPSince arista.cvp collection is in charge of communicating with Cloudvision, we need to emulate it in our testing. So the mock will recreate all cvprac function and use static data coming from CV.So first of all, we need to load MagicMock and cvprac librariesfrom unittest.mock import MagicMock, create_autospecimport pprintimport loggingfrom cvprac.cvp_client import CvpClient, CvpApiNow we can create a Mock from MagickMock. This approach gives us option to create side_effects to define how function should work.def get_cvp_client(cvp_database: MockCVPDatabase) -&amp;gt; MagicMock: # Instantiate a Mock from CvpClient to support authentication process mock_client = create_autospec(CvpClient) # Instantiate a Mock from CvpApi to support CVP interaction mock_client.api = create_autospec(CvpApi) # Create side_effects to expose cvprac methods mock_client.api.get_configlets_and_mappers.side_effect = cvp_database.get_configlets_and_mappers # Expose created MOCK return mock_clientAs you can see in the side_effects definition, we use cvprac method name as it is automatically populated from the create_autospec. In our case, we have an object named cvp_database where we have all the cvprac method we use. Main reason is this object can instantiate a fake database with userâ€™s defined JSON object. Auospeccing also add security guard to recursively speccing attributes and avoid silent error that could break CI.class MockCVPDatabase: &quot;&quot;&quot;Class to mock CVP database being modified during tests&quot;&quot;&quot; def __init__(self, configlets_mappers: dict = None): self.configlets_mappers = configlets_mappers if configlets_mappers is not None else {} This code is not a complete one as it can be a bit too long.And now we have to create our side_effects method. In our example, we will do a side_effect for get_containers methodclass MockCVPDatabase: &quot;&quot;&quot;Class to mock CVP database being modified during tests&quot;&quot;&quot; def get_configlets_and_mappers(self): &quot;&quot;&quot; get_configlets_and_mappers Return Mapping for configlets &quot;&quot;&quot; return self.configlets_mappersThis function exposes data as cvprac would do if it has some real CV connection. So to build this function, you need to define what your JSON structure will be in your MOCK and data structure sent back by cvprac. So in this example, we use the following structure:{ &quot;data&quot;: { &quot;configlets&quot;: [ { &quot;key&quot;: &quot;configlet_0b5f1972-bb04-4d31-931d-baf7061caa13&quot;, &quot;name&quot;: &quot;spine-2-unit-test&quot;, &quot;reconciled&quot;: &quot;false&quot;, &quot;configl&quot;: &quot;alias mock show ip route&quot;, &quot;user&quot;: &quot;cvpadmin&quot;, &quot;note&quot;: &quot;&quot;, &quot;containerCount&quot;: 0, &quot;netElementCount&quot;: 0, &quot;dateTimeInLongFormat&quot;: 1640858616983, &quot;isDefault&quot;: &quot;no&quot;, &quot;isAutoBuilder&quot;: &quot;&quot;, &quot;type&quot;: &quot;Static&quot;, &quot;editable&quot;: &quot;true&quot;, &quot;sslConfig&quot;: &quot;false&quot;, &quot;visible&quot;: &quot;true&quot;, &quot;isDraft&quot;: &quot;false&quot;, &quot;typeStudioConfiglet&quot;: &quot;false&quot; }, //[...] ] }} This output is basically what CV provides when you GET configlet_mappers with curl:curl -X &#39;GET&#39; \\ &#39;https://&amp;lt;cv-ip-address&amp;gt;/cvpservice/configlet/\\ getConfigletsAndAssociatedMappers.do&#39; \\ -H &#39;accept: application/json&#39;Everything seems cool, but how can we use it with Pytest and build relevant test cases ?Configure PytestCreate fixture to instantiate DBLetâ€™s consider writing tests for cv_facts_v3 as it is a fairly easy one.Letâ€™s create a fixture to instantiate environment each time we run a test. This fixture creates an object named database from mock.MockCVPDatabase and uses data from test environment.#!/usr/bin/python# coding: utf-8 -*-from __future__ import (absolute_import, division, print_function)import pytestfrom tests.lib import mock, mock_ansiblefrom tests.data import facts_unitfrom ansible_collections.arista.cvp.plugins.module_utils.facts_tools import CvFactsToolsLOGGER = setup_custom_logger(__name__)@pytest.fixturedef cvp_database(): database = mock.MockCVPDatabase( devices=facts_unit.MOCKDATA_DEVICES, configlets=facts_unit.MOCKDATA_CONFIGLETS, containers=facts_unit.MOCKDATA_CONTAINERS, configlets_mappers=facts_unit.MOCKDATA_CONFIGLET_MAPPERS ) yield database LOGGER.debug(&#39;Final CVP state: %s&#39;, database)This approach is based on a generic content available for all cases. But another approach can be to leverage indirect parametrisation to inject data specific to each test. We wonâ€™t go through each approach here since both presents pro and cons. But if you want to read more, you can refer to this page.Another fixture is used to instantiate arista.cvp utils and is based on the same approach:@pytest.fixture()def fact_unit_tools(request, cvp_database): cvp_client = mock.get_cvp_client(cvp_database) instance = CvFactsTools(cv_connection=cvp_client) LOGGER.debug(&#39;Initial CVP state: %s&#39;, cvp_database) yield instance LOGGER.debug(&#39;Mock calls: %s&#39;, pprint.pformat(cvp_client.mock_calls))As you can see in example above, we log state when we instantiate and release data in fixture. So we can see changes in Cloudvision database and inspect result of tests anytime. Also, yield is header of code runs when we release code after test execution.Parametrize testsPytest aims to create a test we can run against different dataset. These dataset can be pass to the method by using what we call parametrize. In our situation, we use same data we loaded in our fixture so it is easy to validate results.In our example, facts_unit.MOCKDATA_DEVICES is a list, so we do not have to call a method to transform and pytest will iterate to generate 1 test case per entry. Also by default, name for tests genrated by Pytest will have no sense. So we use a function to generate IDs automaticallydef generate_test_ids_dict(val): if &#39;name&#39; in val.keys(): # note this wouldn&#39;t show any hours/minutes/seconds return val[&#39;name&#39;] elif &#39;hostname&#39; in val.keys(): return val[&#39;hostname&#39;]@pytest.mark.parametrize( &quot;device_def&quot;, facts_unit.MOCKDATA_DEVICES, ids=generate_test_ids_dict)build a test caseSo now we are ready to build a test with all these elements: Use fixture to instantiate DB. Use fixture to instantiate a module utils. Use a parametrize to increase test examples.@pytest.mark.usefixtures(&quot;fact_unit_tools&quot;)@pytest.mark.usefixtures(&quot;cvp_database&quot;)@pytest.mark.parametrize( &quot;device_def&quot;, facts_unit.MOCKDATA_DEVICES, ids=generate_test_ids_dict)def test_CvFactsTools__device_get_configlets_with_configlets( fact_unit_tools, device_def): # Only target devices that have configlets configured. # Run command to get configlet for device on Cloudvision result = fact_unit_tools._CvFactsTools__device_get_configlets( netid=device_def[&#39;systemMacAddress&#39;] ) # If we can get the device in the input data (i.e. not from cloudvision), # we can play with assert if device_def[&#39;systemMacAddress&#39;] in [x[&#39;objectId&#39;] for x in facts_unit.MOCKDATA_CONFIGLET_MAPPERS[&#39;data&#39;][&#39;configletMappers&#39;]]: assert len(result) &amp;gt; 0 else: # If not covered by test (i.e. has no configlet attached), we simply skip it pytest.skip(&quot;skipping DEVICES with no configlet&quot;)This example run a method to ask for all configlets for the device and then read the database to compare result with what the method provided.Because fact_unit_tools has been provisioned with a cvp_client based on our mock, the function __device_get_configlets is getting data based on the correct CVP format and well formated by the mock of CVPRAC with get_configlets_and_mappers method.Run testingExecute testing is fairly easy and use basic pytest CLI commands. You donâ€™t have to build a lab somewhere to run unit test of your automation. So CI can catch very quickly any deviation as you can write test for any single method or object you have in your code.pytest \\ -rA -q --cov-report term:skip-covered \\ -v --html=report.html --self-contained-html --cov-report=html \\ --color yes \\ --cov=ansible_collections.arista.cvp.plugins.module_utils \\ --log-cli-level=INFO -m &#39;image&#39; unit/In our case, we can easily test our ansible modules functions before starting to play with ansible. And in reality we do unit test with Pytest &amp;amp; Mock, integration tests with Pytest and CV and finally we can do manual or leverage Molecule to test from Ansibleand a big shout out to the whole ansible-cvp development team: Tamas, Guillaume, Matthieu for ideas, brainstorming, testing, and implementation.Resources Pytest unittest.mock for Pytest What is a Pytest Fixture Parametrizing Tests" }, { "title": "Build Containerlab topology from AVD", "url": "/posts/avd-with-containerlab/", "categories": "Automation", "tags": "ansible, devops, containerlab, arista", "date": "2022-02-04 15:09:00 +0100", "snippet": "OverviewUntil recently, if we want to do networking lab, we had option to play with GNS3 or EVE-NG with Virtual Machine for Network OS. But 2021 has seen the rise of Containerlab which gives us ability to play Lab As Code as you can define your own topology in a YAML file.Arista cEOS is natively supported by the tool and gives you opportunity to build lab easily from any type of tool in charge of your design. Big thanks you to Nokiaâ€™s team for making this tool fully open-source and with support for all NOS.In this article, we will see how to build a containerlab topology from an Arista Validated Design collection for ansible where you define your EVPN topology.Yes, with AVD, you can easily build any type of EVPN/VXLAN fabric, but at some point it is nice to test configuration and building topology on the fly would make design easier.Of course, you can also find some containerlab examples with Arista cEOS on github, but here we are going to talk about generating lab from Ansible.AVD to design an EVPN fabricThis guide is not a detailed guide to setup Arista AVD collection. For this type of document, you should visit specific documentation for that. Here we will build a basic L3LS EVPN fabric based on 2 spines, 4 groups of 2 vteps and some L2 edge switches.Install environmentHere we are talking about python for ansible and Golang for Containerlab. And obviously, you need a docker engine to bring up lab at the end.# Download AVD requirements$ curl -fsSL https://raw.githubusercontent.com/aristanetworks/ansible-avd/devel/ansible_collections/arista/avd/requirements.txt$ pip3 install -r requirements.txt# Install ansible$ pip3 install ansible-coreOnce ansible is installed, you can use galaxy to install AVD and avd_to_containerlab role:$ ansible-galaxy collection install arista.avd$ ansible-galaxy collection install titom73.avd_toolsNow we can install Containerlab:$ bash -c &quot;$(curl -sL https://get-clab.srlinux.dev)&quot;Since we are here, letâ€™s download a cEOS version from Arista website. For that, you can use eos-download as explained in this post. And finally, install it on your Machine.$ docker import cEOS-lab.tar.xz ceosimage:&amp;lt;your-ceos-version&amp;gt;Now we are ready to go to the build phase.Ansible configurationTopology file is provided below and will be the key to build containerlab topology Spine topology:spine: defaults: platform: vEOS-LAB bgp_as: 65001 loopback_ipv4_pool: 192.168.1.0/24 bgp_defaults: &#39;&#39; mlag_peer_ipv4_pool: 172.31.253.0/31 mlag_peer_l3_ipv4_pool: 172.31.253.2/31 nodes: ceos-spine1: id: 1 mgmt_ip: 10.73.255.101/24 mac_address: &#39;0c:1d:c0:1d:62:01&#39; ceos-spine2: id: 2 mgmt_ip: 10.73.255.102/24 mac_address: &#39;0c:1d:c0:1d:62:02&#39; vTEP (aka L3LEAF) topologyl3leaf: defaults: loopback_ipv4_pool: 192.168.255.0/24 loopback_ipv4_offset: 2 vtep_loopback_ipv4_pool: 192.168.254.0/24 uplink_interfaces: [&#39;Ethernet1&#39;, &#39;Ethernet2&#39;] uplink_switches: [&#39;ceos-spine1&#39;, &#39;ceos-spine2&#39;] uplink_ipv4_pool: 172.31.255.0/24 evpn_route_servers: [&#39;ceos-spine1&#39;, &#39;ceos-spine2&#39;] mlag_peer_ipv4_pool: 172.31.253.0/31 mlag_peer_l3_ipv4_pool: 172.31.253.2/31 mlag_interfaces: &#39;&#39; virtual_router_mac_address: 00:1c:73:00:dc:01 bgp_defaults: &#39;&#39; spanning_tree_priority: 4096 spanning_tree_mode: mstp node_groups: ceos_leaf1: bgp_as: 65101 filter: tenants: [ &#39;Tenant_A&#39;, &#39;Tenant_B&#39; ] tags: [ &#39;POD01&#39;, &#39;DC1&#39; ] nodes: ceos-leaf1a: id: 11 mgmt_ip: 10.73.255.111/24 uplink_switch_interfaces: [Ethernet1, Ethernet1] mac_address: &#39;0c:1d:c0:1d:62:11&#39; ceos-leaf1b: id: 12 mgmt_ip: 10.73.255.112/24 uplink_switch_interfaces: [Ethernet2, Ethernet2] mac_address: &#39;0c:1d:c0:1d:62:12&#39;# [ Output ommitted for readability]# ... L2 Switch (aka L2LEAF) topologyedge: defaults: uplink_switches: [&#39;ceos-leaf1a&#39;, &#39;ceos-leaf1b&#39;] uplink_interfaces: [&#39;Ethernet1&#39;, &#39;Ethernet2&#39;] mlag_peer_ipv4_pool: 172.31.253.0/31 mlag_peer_l3_ipv4_pool: 172.31.253.2/31 node_groups: ceos_edge_leaf1: uplink_switches: [ ceos-leaf1a, ceos-leaf1b ] filter: tenants: [ &#39;Tenant_A&#39;, &#39;Tenant_B&#39; ] tags: [ &#39;POD01&#39;, &#39;DC1&#39; ] nodes: ceos-agg01: id: 21 mgmt_ip: 10.73.255.121/24 mac_address: &#39;0c:1d:c0:1d:62:21&#39; uplink_switch_interfaces: [ Ethernet5, Ethernet5 ]# [ Output ommitted for readability]# ...For a complete list of options, it is better to read the full documentation of AVD.Also, full inventory is available on github. So you can just clone and play.PlaybookAfter inventory has been created, we can create a playbook. This one will be fairly easy and will do 3 things: Transform abstracted topology into YAML EOS configuration file Generate EOS Cli configuration file Generate a containerlab topology loading AVD generated configuration files.So here we go:# playbook/build.yml---- name: Build Switch configuration hosts: [all] tags: - avd collections: - arista.avd tasks: - name: generate intended variables tags: [build] import_role: name: eos_designs - name: generate device intended config and documentation tags: [build] import_role: name: eos_cli_config_gen- name: Build containerlab topology hosts: [all] connection: local gather_facts: false collections: - arista.avd - titom73.avd_tools tasks: - name: &#39;Build Containerlab Topology&#39; tags: - containerlab import_role: name: inetsix.avd_tools.eos_designs_to_containerlab vars: mgmt_network_v4: 10.73.255.0/24 ceos_version: arista/ceos:4.27.1F eapi_base: 8000As you can see, eos_designs_to_containerlab role needs some elements to generate a full configuration: mgmt_network_v4: Management network to configure on docker side. It must be the same as the one used in AVD eapi_base: Base port for eAPI to expose eAPI service to the network outside of docker. ceos_version: Which version of cEOS to use.Make the magic happens$ ansible-playbook playbooks/containerlab-build.yml -i containerlab/inventory.yml[...]===============================================================================arista.avd.eos_cli_config_gen : Generate content of device documentation ---------- 4.61sarista.avd.eos_cli_config_gen : Generate eos intended configuration --------------- 3.30sarista.avd.eos_designs : Generate device configuration in structured format ------- 1.75sarista.avd.eos_cli_config_gen : Generate TOC for device documentation ------------- 0.92sarista.avd.eos_designs : Write device structured configuration to YAML file ------- 0.85sarista.avd.eos_designs : Generate fabric documentation in Markdown Format. -------- 0.72sarista.avd.eos_cli_config_gen : Create required output directories if not present - 0.61sarista.avd.eos_designs : Set AVD facts -------------------------------------------- 0.58sarista.avd.eos_cli_config_gen : Store checksum of existing device documentation --- 0.50sarista.avd.eos_designs : Generate TOC for fabric documentation -------------------- 0.47sarista.avd.eos_designs : Generate fabric topology in csv format. ------------------ 0.46sinetsix.avd_tools.eos_designs_to_containerlab : Generate containerlab topology ---- 0.45sarista.avd.eos_designs : Create required output directories if not present -------- 0.44sarista.avd.eos_designs : Set AVD topology facts ----------------------------------- 0.28sarista.avd.eos_designs : Store checksum of existing fabric documentation ---------- 0.26sPlaybook run took 0 days, 0 hours, 0 minutes, 17 secondsSo now you can look at files generated: EOS configuration files under intended/configs Containerlab topology in containerlab/containerlab.yml---name: ceos_fabricmgmt: network: &#39;mgmt_ceos_fabric&#39; ipv4_subnet: 10.73.255.0/24topology: kinds: ceos: image: arista/ceos:4.27.1F nodes: ceos-agg01: image: arista/ceos:4.27.1F mgmt_ipv4: 10.73.255.121 kind: ceos startup-config: &amp;lt;path&amp;gt;/intended/configs/ceos-agg01.cfg ports: - 8021:443/tcp env: TMODE: lacp# [...] links: - endpoints: [&quot;ceos-agg01:eth1&quot;, &quot;ceos-leaf1a:eth5&quot;]# [...]And you can start containerlab:$ sudo containerlab deploy --topo inventories/containerlab/containerlabs.yml[sudo] password for tom:INFO[0000] Parsing &amp;amp; checking topology file: containerlabs.ymlINFO[0000] Creating lab directory: /home/tom/arista-\\ avd/avd-lab-validation/clab-ceos_fabricINFO[0000] Creating docker network: Name=&#39;mgmt_ceos_fabric&#39;, \\ IPv4Subnet=&#39;10.73.255.0/24&#39;, IPv6Subnet=&#39;&#39;, MTU=&#39;1500&#39;INFO[0000] config file &#39;/home/tom/arista-avd/avd-lab-validation/\\ clab-ceos_fabric/ceos-agg01/flash/startup-config&#39; for node \\ &#39;ceos-agg01&#39; already exists and will not be generated/resetINFO[0000] Creating container: ceos-agg01[...]INFO[0004] Creating virtual wire: ceos-leaf3a:eth3 &amp;lt;--&amp;gt; ceos-leaf4a:eth3INFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-leaf2a&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-bl01a&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-leaf3a&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-leaf4a&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-agg02&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-leaf1a&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-leaf2b&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-leaf1b&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-spine1&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-spine2&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-bl01b&#39; nodeINFO[0005] Running postdeploy actions for Arista cEOS &#39;ceos-agg01&#39; nodeOnce it is over, you should see output below. It gives you some information such as kind, version and management 0 IP address.INFO[0143] ðŸŽ‰ New containerlab version 0.23.0 is available! Release notes: https://containerlab.srlinux.dev/rn/0.23/Run &#39;containerlab version upgrade&#39; to upgrade or go check other installation options at https://containerlab.srlinux.dev/install/+----+------------------------------+--------------+---------------------+------+---------+------------------+--------------+| # | Name | Container ID | Image | Kind | State | IPv4 Address | IPv6 Address |+----+------------------------------+--------------+---------------------+------+---------+------------------+--------------+| 1 | clab-ceos_fabric-ceos-agg01 | 37973cf400d7 | arista/ceos:4.27.1F | ceos | running | 10.73.255.121/24 | N/A || 2 | clab-ceos_fabric-ceos-agg02 | cb37f8b37e0f | arista/ceos:4.27.1F | ceos | running | 10.73.255.122/24 | N/A || 3 | clab-ceos_fabric-ceos-bl01a | 9ba07822490b | arista/ceos:4.27.1F | ceos | running | 10.73.255.115/24 | N/A || 4 | clab-ceos_fabric-ceos-bl01b | 40eaf38dd58c | arista/ceos:4.27.1F | ceos | running | 10.73.255.116/24 | N/A || 5 | clab-ceos_fabric-ceos-leaf1a | f4ac53c2ae4d | arista/ceos:4.27.1F | ceos | running | 10.73.255.111/24 | N/A || 6 | clab-ceos_fabric-ceos-leaf1b | c03adbff83af | arista/ceos:4.27.1F | ceos | running | 10.73.255.112/24 | N/A || 7 | clab-ceos_fabric-ceos-leaf2a | b59260c441a0 | arista/ceos:4.27.1F | ceos | running | 10.73.255.113/24 | N/A || 8 | clab-ceos_fabric-ceos-leaf2b | 5514405b6429 | arista/ceos:4.27.1F | ceos | running | 10.73.255.114/24 | N/A || 9 | clab-ceos_fabric-ceos-leaf3a | 87e22287c2f2 | arista/ceos:4.27.1F | ceos | running | 10.73.255.117/24 | N/A || 10 | clab-ceos_fabric-ceos-leaf4a | 878b75601225 | arista/ceos:4.27.1F | ceos | running | 10.73.255.118/24 | N/A || 11 | clab-ceos_fabric-ceos-spine1 | 7d10d85458e0 | arista/ceos:4.27.1F | ceos | running | 10.73.255.101/24 | N/A || 12 | clab-ceos_fabric-ceos-spine2 | 7e0effa1ccd6 | arista/ceos:4.27.1F | ceos | running | 10.73.255.102/24 | N/A |+----+------------------------------+--------------+---------------------+------+---------+------------------+--------------+Also, you can look at docker directly. This one is interesting to ge eAPI port mapping:docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES87e22287c2f2 arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8015-&amp;gt;443/tcp, :::8015-&amp;gt;443 tcp clab-ceos_fabric-ceos-leaf3af4ac53c2ae4d arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8011-&amp;gt;443/tcp, :::8011-&amp;gt;443/tcp clab-ceos_fabric-ceos-leaf1a5514405b6429 arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8014-&amp;gt;443/tcp, :::8014-&amp;gt;443/tcp clab-ceos_fabric-ceos-leaf2b40eaf38dd58c arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8018-&amp;gt;443/tcp, :::8018-&amp;gt;443/tcp clab-ceos_fabric-ceos-bl01bb59260c441a0 arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8013-&amp;gt;443/tcp, :::8013-&amp;gt;443/tcp clab-ceos_fabric-ceos-leaf2acb37f8b37e0f arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8022-&amp;gt;443/tcp, :::8022-&amp;gt;443/tcp clab-ceos_fabric-ceos-agg02c03adbff83af arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8012-&amp;gt;443/tcp, :::8012-&amp;gt;443/tcp clab-ceos_fabric-ceos-leaf1b7d10d85458e0 arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8001-&amp;gt;443/tcp, :::8001-&amp;gt;443/tcp clab-ceos_fabric-ceos-spine1878b75601225 arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8016-&amp;gt;443/tcp, :::8016-&amp;gt;443/tcp clab-ceos_fabric-ceos-leaf4a7e0effa1ccd6 arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8002-&amp;gt;443/tcp, :::8002-&amp;gt;443/tcp clab-ceos_fabric-ceos-spine237973cf400d7 arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8021-&amp;gt;443/tcp, :::8021-&amp;gt;443/tcp clab-ceos_fabric-ceos-agg019ba07822490b arista/ceos:4.27.1F &quot;/sbin/init systemd.â€¦&quot; 2 minutes ago Up 2 minutes 0.0.0.0:8017-&amp;gt;443/tcp, :::8017-&amp;gt;443/tcp clab-ceos_fabric-ceos-bl01aAnd you can connect to your EOS cli directly from host with docker command:docker exec -it clab-ceos_fabric-ceos-spine1 Cliceos-spine1&amp;gt;ceos-spine1&amp;gt;enceos-spine1#show lldp neighborsLast table change time : 0:12:19 agoNumber of table inserts : 20Number of table deletes : 1Number of table drops : 0Number of table age-outs : 1Port Neighbor Device ID Neighbor Port ID TTL---------- ------------------------ ---------------------- ---Et1 ceos-leaf1a Ethernet1 120Et2 ceos-leaf1b Ethernet1 120Et3 ceos-leaf2a Ethernet1 120Et4 ceos-leaf2b Ethernet1 120Et5 ceos-bl01a Ethernet1 120Et6 ceos-bl01b Ethernet1 120Et7 ceos-leaf3a Ethernet1 120Et8 ceos-leaf4a Ethernet1 120ceos-spine1#show bgp evpn summaryBGP summary information for VRF defaultRouter identifier 192.168.1.1, local AS number 65001Neighbor Status Codes: m - Under maintenance Description Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc ceos-leaf1a 192.168.255.13 4 65101 46 42 0 0 00:13:01 Estab 10 10 ceos-leaf1b 192.168.255.14 4 65101 46 42 0 0 00:13:01 Estab 10 10 ceos-leaf2a 192.168.255.15 4 65102 51 41 0 0 00:13:07 Estab 10 10 ceos-leaf2b 192.168.255.16 4 65102 52 42 0 0 00:13:08 Estab 10 10 ceos-leaf3a 192.168.255.17 4 65103 52 44 0 0 00:13:16 Estab 5 5 ceos-leaf4a 192.168.255.18 4 65104 53 48 0 0 00:13:28 Estab 5 5 ceos-bl01a 192.168.255.19 4 65105 52 48 0 0 00:13:18 Estab 4 4 ceos-bl01b 192.168.255.20 4 65105 54 48 0 0 00:13:18 Estab 4 4If you change your AVD parameter, your lab will be fully rebuild and will be ready for testing. You just have to destroy active topology before running again the playbook.Resources Containerlab / User Guide Containerlab examples Arista EOS AVD Topology examples Arista AVD ansible Collection: Documentation / Repository Collection to map AVD with Containerlab User journey &amp;amp; experiences with Containerlab by @Julio" }, { "title": "Deploy &amp; Configure AWX with Ansible EE", "url": "/posts/Ansible-AWX-EE/", "categories": "Automation", "tags": "ansible, devops, awx, arista", "date": "2022-02-03 00:00:00 +0100", "snippet": "AboutThis example shows how to deploy basic EVPN/VXLAN Fabric based on Arista Validated Design roles using Ansible Tower/AWX. This repository will be used as project on AWX and we will describe how to configure Tower for the following topics: Create a project Create inventory Install collections Install python requirementsDisclaimerThis guide was originally published in our Arista AVD ecosystem and is based on field experience and it is not considered as an official AWX/Tower design guide. All the resources used in this post are available in the following repository.It is not a post about how to use Arista Validated Design collection, but how to configure Ansible AWX to use it.Before startingIf you want to see how to build your inventory and all related variables, it is recommended to read following documentation: How to start L3LS EVPN Abstraction roleThis guide describe how to install and configure AWX to run Arista AVD ansible collection using official approach as per AWX repository and requires to have a Kubernetes cluster available to install awx operator.RequirementsTo play with this repository, you need: A kubernetes cluster set up and ready to use. AWX Operator repository uses minikube, but any flavor can be used. A docker engine or podman to build Ansible Execution Engine.AWX InstallationDeploy AWX OperatorIf you do not have installed AWX operator yet, you can install it with the following commands:# Clone repository$ git clone https://github.com/ansible/awx-operator.git# Create namespace in kubernetes$ kubectl create namespace awx-avd-demonamespace/awx-avd-demo createdkubectl config set-context --current --namespace=awx-avd-demoContext &quot;minikube&quot; modified# Deploy operator$ cd awx-operator$ export NAMESPACE=awx-avd-demo$ make deployFull step by step is available on AWX Operator repositoryDeploy an AWX instanceAll the following steps will be executed in this repository as it provides both ansible content and AWX deployment manifestAWX manifest# manifests/awx-instance.yml---apiVersion: awx.ansible.com/v1beta1kind: AWXmetadata: name: awx-for-avd-demospec: service_type: nodeportAWX deploymentTo deploy AWX, just run the following command:# Check operator is active$ kubectl get podsNAME READY STATUS RESTARTS AGEawx-operator-controller-manager-6d959bd7dd-nwjz8 2/2 Running 0 6m54s# Deploy AWX$ kubectl apply -f manifests/awx-instance.yml# Monitor deployment (it can take time to appear)$ kubectl get pods -l &quot;app.kubernetes.io/managed-by=awx-operator&quot;NAME READY STATUS RESTARTS AGEawx-demo-postgres-0 1/1 Running 0 24sawx-demo-6f58cd7b8d-6dpwr 4/4 Running 0 6sOnce container are UP and running, you should monitor logs to check provisioning completion:$ kubectl logs -f deployments/awx-operator-controller-manager -c awx-manager...PLAY RECAP *********************************************************************localhost : ok=62 changed=0 unreachable=0 failed=0 skipped=45 rescued=0 ignored=0Get access informationAWX instance is available via a node port. So you can use following command:# For minikubeminikube service awx-demo-service --url -n $NAMESPACE# For other flavors$ kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEawx-operator-controller-manager-metrics-service ClusterIP 10.152.183.37 &amp;lt;none&amp;gt; 8443/TCP 14mawx-demo-postgres ClusterIP None &amp;lt;none&amp;gt; 5432/TCP 6m35sawx-demo-service NodePort 10.152.183.71 &amp;lt;none&amp;gt; 80:31025/TCP 6m19s In this example, instance is listening on port 31025AWX Credentials are admin and password generated by Kuberneteskubectl get secret awx-demo-admin-password -o jsonpath=&quot;{.data.password}&quot; | base64 --decodeO2WBkBTW7CKWUZLqm263PklCL5m7K0GUConfigure AWXCreate Ansible Execution EnvironmentAnsible has recently introduced Execution Environment which is basically a container to execute your playbooks. The main interest is you donâ€™t have to build a virtual environment in AWX.To build such container, you need docker or podman as well as ansible-builder.pip install ansible-builderAnd then you have to define your builder file:---version: 1build_arg_defaults: EE_BASE_IMAGE: &#39;quay.io/ansible/ansible-runner:stable-2.12-devel&#39;dependencies: galaxy: requirements.yml python: requirements.txtadditional_build_steps: prepend: | RUN pip install --upgrade pip setuptools RUN yum install -y \\ make \\ wget \\ curl \\ less \\ git \\ zsh \\ vim \\ sshpass Note that collection definition is part of requirements.yml. So a new image should be build each time you want to upgrade to a new avd or cvp collection.To build image, nothing complex:ansible-builder -f exeution-environment.yml -t &amp;lt;your-docker-image:tag&amp;gt;Ansible version for runner can be found in ansible-runner registryAlso upload image to a registry.docker push &amp;lt;your-docker-image:tag&amp;gt;You can read more in this post.Install Ansible Execution EnvironmentAfter your image has been uploaded on a public or private registry, you can define this Execution Environment in AWX (Administration / Execution Environments) If your image is on a private registry, you have to create credentials for your registryConfigure a ProjectNow we will use this repository as source for both playbooks and inventory. Go to Resources / Projects and select AddThis project will be used for 2 things: Get our inventory and all attached variables. Get our playbooks to run in AWX.Configure project with: SCM Type: Git SCM Branch: master Ansible Environment: /your/path/to/venv SCM URL: https://github.com/arista-netdevops-community/avd-with-ansible-tower-awx.gitDonâ€™t forget the following elements: Set correct Execution Environment from the list. Select correct branch Configure optional credentials if requiredCreate inventoryWe can now create inventory in AWX in Resources / Inventories and select Add InventoryClick Save and and then on SourcesAnd then, complete information:Create Template (aka Playbook)Template is in charge of the glue between inventory, execution environment and playbook to run.Go to Resources / Templates and select Add Job TemplateIn this section, feel free to use your tags based on your need. Here playbook will execute only build and not deploy and will skip documentation.Whatâ€™s next ?Now everything is set and you should be able to run your playbook or build your own workflow !Resources Play with Ansible Execution Environment Ansible Arista Validated Design repository. Ansible Arista CloudVision Collection repository. AWX Operator repository Minikube Ansible builder" }, { "title": "eos-download introduction", "url": "/posts/eos-download/", "categories": "Arista", "tags": "arista, python", "date": "2022-02-01 00:00:00 +0100", "snippet": "In this post, we will see how to use eos-download script to automate your EVE-NG instance with new Arista EOS version for your labs.One of the pain point when you want to test a design is to prepare VMs with correct software version and do the provisioning. So eos-download tries to help provisioning software in EVE-NG to let you start quickly to work on your network things.Install scriptScript installation is based on standard python packaging mechanism and can be installed with either setup_tools or pip. All the code is python &amp;gt;=3.8 compliant.$ pip3 install git+https://github.com/titom73/arista-downloaderFrom here, your script is installed in your $PATH and can be called from your shell. Even if you can install it on any device using python, we will do that on EVE-NG server as our goal here is to provision VM on EVE-NG, but you can also use that script for general purpose.ConfigurationThere is no configuration file to create and everything is done by CLI argument. Besides that, some options use environment variables to reduce size of the CLI and make process easier. it is the case for token authentication.Talking about authentication, you need to get a token to access Arista download site. You can generate this token in your account page.UsageGeneric usageeos-download script provides only few options:eos-download -husage: eos-download [-h] [--token TOKEN] --version VERSION [--image IMAGE] [--destination DESTINATION] [--eve] [--noztp] [--verbose VERBOSE]EOS downloader script.optional arguments: -h, --help show this help message and exit --token TOKEN arista.com user API key - can use ENV:ARISTA_TOKEN --image IMAGE Type of EOS image required --version VERSION EOS version to download from website --destination DESTINATION Path where to save EOS package downloaded --eve Option to install EOS package to EVE-NG --noztp Option to deactivate ZTP when used with EVE-NG --verbose VERBOSE Script verbosityAs explained earlier, this script can be used for general purposes and gives you option to download almost any flavor of EOS (--image) from Arista website: International version (--image INT) 64 bits version (--image 64) 2GB flash platform (--image 2GB) 2GB running International (--image 2GB-INT) Virtual EOS image (--image vEOS) Virtual Lab EOS (--image vEOS-lab) Virtual Lab EOS running 64B (--image vEOS64-lab) Docker version of EOS (--image cEOS) Docker version of EOS running in 64 bits (--image cEOS64)In our case, we will just download vEOS-LAB flavorFor the version, we will download EOS in version 4.25.1F. So the easiest CLI will be:root@eve-ng:~/demo# eos-download \\ --token fake-token-here \\ --image vEOS-lab \\ --version 4.25.1F2022-02-03 12:52:34.135 | INFO | eos_downloader:authenticate:360 - Authenticated on arista.com2022-02-03 12:52:35.023 | INFO | eos_downloader:_parse_xml:122 - Found vEOS-lab-4.25.1F.vmdk at /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.1F/vEOS-lab/vEOS-lab-4.25.1F.vmdk2022-02-03 12:52:35.024 | INFO | eos_downloader:_download_file:314 - File found on arista server: /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.1F/vEOS-lab/vEOS-lab-4.25.1F.vmdk2022-02-03 12:52:58.429 | INFO | eos_downloader:_parse_xml:122 - Found vEOS-lab-4.25.1F.vmdk.sha512sum at /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.1F/vEOS-lab/vEOS-lab-4.25.1F.vmdk.sha512sum2022-02-03 12:53:00.277 | WARNING | eos_downloader:download_local:399 - Downloaded file is correct.root@eve-ng:~/demo#As you can see, script authenticate against Arista website, finds the link for download and do a sha512 comparison to validate download is not corrupted.But file is downloaded in your current location:root@eve-ng:~/demo# lsvEOS-lab-4.25.1F.vmdk vEOS-lab-4.25.1F.vmdk.sha512sumroot@eve-ng:~/demo#Ok, but what if we donâ€™t want to type our token each time ? Then, just configure an environment variable automatically loaded by the script:# Configure your token. it can also be something installed in your .bashrc / .zshrcroot@eve-ng:~/demo# export ARISTA_TOKEN=fake-token-here# Run scriptroot@eve-ng:~/demo# eos-download \\ --image vEOS-lab \\ --version 4.25.1F2022-02-03 12:52:34.135 | INFO | eos_downloader:authenticate:360 - Authenticated on arista.com2022-02-03 12:52:35.023 | INFO | eos_downloader:_parse_xml:122 - Found vEOS-lab-4.25.1F.vmdk at /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.1F/vEOS-lab/vEOS-lab-4.25.1F.vmdk2022-02-03 12:52:35.024 | INFO | eos_downloader:_download_file:314 - File found on arista server: /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.1F/vEOS-lab/vEOS-lab-4.25.1F.vmdk2022-02-03 12:52:58.429 | INFO | eos_downloader:_parse_xml:122 - Found vEOS-lab-4.25.1F.vmdk.sha512sum at /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.1F/vEOS-lab/vEOS-lab-4.25.1F.vmdk.sha512sum2022-02-03 12:53:00.277 | WARNING | eos_downloader:download_local:399 - Downloaded file is correct.root@eve-ng:~/demo#Download &amp;amp; Install in EVE-NGTo install image in EVE-NG in just one command, just use the following one:root@eve-ng:~/demo# eos-download \\ --token fake-token-here \\ --image vEOS-lab \\ --version 4.25.7M \\ --eve2022-02-03 12:57:02.462 | INFO | eos_downloader:authenticate:360 - Authenticated on arista.com2022-02-03 12:57:03.373 | INFO | eos_downloader:_parse_xml:122 - Found vEOS-lab-4.25.7M.vmdk at /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.7M/vEOS-lab/vEOS-lab-4.25.7M.vmdk2022-02-03 12:57:03.374 | INFO | eos_downloader:_download_file:314 - File found on arista server: /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.7M/vEOS-lab/vEOS-lab-4.25.7M.vmdk2022-02-03 12:57:25.747 | INFO | eos_downloader:provision_eve:418 - Converting VMDK to QCOW2 format2022-02-03 12:57:26.353 | INFO | eos_downloader:provision_eve:420 - Applying unl_wrapper to fix permissionsFeb 03 12:57:26 Feb 03 12:57:26 Online Check state: ValidAs you can, your EOS image is automatically available in your EVE-NG interface:But this image still has ZTP configured. It means during the startup process, all interfaces will be active to send DHCP request and get information to find startup configuration or register to a Cloudvision instance.But sometimes, we need VM to boot and get pre-configured configuration set in EVE-NG. For that, just use flag --noztp in the scriptroot@eve-ng:~/demo# eos-download \\ --token fake-token-here \\ --image vEOS-lab \\ --version 4.25.7M \\ --eve \\ --noztp2022-02-03 13:03:03.384 | INFO | eos_downloader:authenticate:360 - Authenticated on arista.com2022-02-03 13:03:03.883 | INFO | eos_downloader:_parse_xml:122 - Found vEOS-lab-4.25.7M.vmdk at /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.7M/vEOS-lab/vEOS-lab-4.25.7M.vmdk2022-02-03 13:03:03.883 | INFO | eos_downloader:_download_file:314 - File found on arista server: /support/download/EOS-USA/Active Releases/4.25/EOS-4.25.7M/vEOS-lab/vEOS-lab-4.25.7M.vmdk2022-02-03 13:03:24.189 | INFO | eos_downloader:provision_eve:418 - Converting VMDK to QCOW2 format2022-02-03 13:03:24.829 | INFO | eos_downloader:provision_eve:420 - Applying unl_wrapper to fix permissionsFeb 03 13:03:24 Feb 03 13:03:24 Online Check state: Valid2022-02-03 13:03:32.779 | INFO | eos_downloader.eos:_disable_ztp:52 - Mounting volume to disable ZTP2022-02-03 13:03:41.252 | INFO | eos_downloader.eos:_disable_ztp:61 - Unmounting volume in /opt/unetlab/addons/qemu/veos-lab-4.25.7m-noztp2022-02-03 13:03:41.290 | INFO | eos_downloader.eos:_disable_ztp:64 - Volume has been successfully unmounted at /opt/unetlab/addons/qemu/veos-lab-4.25.7m-noztpAnd then, your image is installed with a suffix configured to -noztp and also available in EVE-NG interface:ConclusionSo here you go, now you are ready to lab !And of course, feel free to report any issue or feature request in the repository" }, { "title": "Build an Ansible Execution Environment", "url": "/posts/Ansible-ee/", "categories": "Automation", "tags": "ansible, devops, awx", "date": "2022-02-01 00:00:00 +0100", "snippet": "OverviewRecently, Ansible has released a tool to build docker image with all your requirements. Main goal is to support execution of projects in Ansible AWX and to replace the need to configure Python virtual-environment.In addition, it is very convenient to leverage this function to build test environment for your ansible development or to ship a feature for testing.In this article, we will see how to build such images and how to use these images.Pre-requisitesBefore we start, you need to have a docker or a podman running on your machine. Docker desktop PodmanAlso python in version 3.x is highly recommended.Configure your Ansible Execution EnvironmentInstead of using Dockerfile syntax, ansible EE is based on a very simple YAML schema to define following elements: Base image from ansible to use: It is where you will specify your ansible version List of python and ansible requirements. A list of Dockerfile commands to run before and/or after the main part of the buildSnippet below is an example of such file:# execution-environment.yml---version: 1build_arg_defaults: EE_BASE_IMAGE: &#39;quay.io/ansible/ansible-runner:stable-2.12-devel&#39;dependencies: galaxy: requirements.yml python: requirements.txtadditional_build_steps: prepend: | RUN pip install --upgrade pip setuptools RUN yum install -y make wget curl less git vim sshpass # append: # - RUN ls -la /etcAnd in the same directory, you also have to put your requirements files:ls -lexecution-environment.ymlrequirements.txtrequirements.ymlWe wonâ€™t go through the python requirements file as it is a standard one, but we can take few seconds to look at the Ansible requirement file:---collections: - name: titom73.avd_tools version: 0.2.0 - name: ansible.netcommon - ansible.posix - community.generalAs you can see, we can optionally defined version for collection. And we can also use git as source of installation as shown in example below:collections: - name: arista.avd source: git+https://github.com/titom73/ansible-inetsix.git#/ansible_collections/titom73/avd_tools/,develAs you can see, URL is specific and contains the following blocks: Git repo: git+https://github.com/aristanetworks/ansible-avd.git Path for the collection: #/ansible_collections/arista/avd/ Git Branch to consider: ,devel More information available on Ansible websiteBuild Ansible Execution EnvironmentTo build this image, you need to install a tool from ansible named ansible-builder available via pip$ pip3 install ansible-builderThis tool is in charge to read your configuration file, generate a Containerfile or a Dockerfile and execute the build using either podman (default) or docker with tag you will provide via the CLI.Build using podman# Build image$ ansible-builder build -f execution-environment.yml \\ --tag titom73/ansible-ee:2.12Running command: podman build -f context/Containerfile -t titom73/ansible-ee:2.12 contextComplete! The build context can be found at: \\ /Users/titom73/Projects/avd-stack/avd-lab-validation/context# Check resultpodman imagesREPOSITORY TAG IMAGE ID CREATED SIZElocalhost/titom73/ansible-ee 2.12 9fd6fc5264f4 54 seconds ago 1.15 GBquay.io/ansible/ansible-runner stable-2.12-devel 5e00350a4bb5 20 hours ago 881 MB# Check your requirements are installedpodman container run -it --rm \\ localhost/titom73/ansible-ee:2.12 ansible-galaxy collection list# /usr/share/ansible/collections/ansible_collectionsCollection Version----------------- -------ansible.netcommon 2.5.0ansible.posix 1.3.0ansible.utils 2.4.3community.general 4.4.0titom73.avd_tools 0.2.0Build with Docker engineProcess is exactly the same, except you have to configure --container-runtime option to use docker# Build image$ ansible-builder build -f execution-environment.yml \\ --tag titom73/ansible-ee:2.12 \\ --container-runtime dockerRunning command: docker build -f context/Dockerfile -t titom73/ansible-ee:2.12 context# Check resultdocker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtitom73/ansible-ee 2.12 fdcdd11ea397 About a minute ago 1.11GB# Check your requirements are installeddocker run -it --rm \\ localhost/titom73/ansible-ee:2.12 ansible-galaxy collection list# /usr/share/ansible/collections/ansible_collectionsCollection Version----------------- -------ansible.netcommon 2.5.0ansible.posix 1.3.0ansible.utils 2.4.3community.general 4.4.0titom73.avd_tools 0.2.0Hack the build.This section is more about tips &amp;amp; tricks around ansible-builderBuild container with local collectionIf you are developing a feature and need someone to test it, you can build image with your local collection# execution-environment.yml---version: 1build_arg_defaults: EE_BASE_IMAGE: &#39;quay.io/ansible/ansible-runner:latest&#39;dependencies: galaxy: requirements.yml python: requirements.txtadditional_build_steps: prepend: | RUN pip install --upgrade pip setuptools RUN yum install -y make wget curl less git vim sshpass append: - COPY ansible_collections/titom73/avd_tools/ \\ /usr/share/ansible/collections/ansible_collections/titom73/avd_tools - RUN ls -la /usr/share/ansible/collections/ansible_collections/titom73 Assuming path to your local collection is ansible_collections/titom73/avd_tools/Generic MakeEE_IMAGE ?= titom73/ansible-eeEE_TAG ?= 2.12.PHONY: ee-buildee-build: ## Build Ansible Execution Builder ansible-builder build \\ --tag $(EE_IMAGE):$(EE_TAG) \\ --container-runtime docker \\ -f $(EE_FILE) \\ --build-arg EE_BASE_IMAGE=quay.io/ansible/ansible-runner:stable-$(EE_TAG)-develSo easy to use:make ee-build EE_TAG=2.11Additional Resources Ansible Builder documentation Network to Code: Ansible Builder, Runner, and Execution Environments Ansible Runner documentation" } ]
